{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Chat Bot - Training Deep Learning Model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all the Libraries\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dropout, Dense\n",
    "from keras.optimizers import SGD\n",
    "from keras.callbacks import EarlyStopping\n",
    "import random\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "word_file = open('C:/Users/mamoo/Desktop/Python_Coding/ChatBot/intents.json').read()\n",
    "intents = json.loads(word_file)\n",
    "#The following need to be downloaded the first time only!\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hi', 'there'] greeting\n",
      "['How', 'are', 'you'] greeting\n",
      "['Is', 'anyone', 'there', '?'] greeting\n",
      "['Hey'] greeting\n",
      "['Hola'] greeting\n",
      "['Hello'] greeting\n",
      "['Good', 'day'] greeting\n",
      "['Bye'] goodbye\n",
      "['See', 'you', 'later'] goodbye\n",
      "['Goodbye'] goodbye\n",
      "['Nice', 'chatting', 'to', 'you', ',', 'bye'] goodbye\n",
      "['Till', 'next', 'time'] goodbye\n",
      "['Thanks'] thanks\n",
      "['Thank', 'you'] thanks\n",
      "['That', \"'s\", 'helpful'] thanks\n",
      "['Awesome', ',', 'thanks'] thanks\n",
      "['Thanks', 'for', 'helping', 'me'] thanks\n",
      "['How', 'you', 'could', 'help', 'me', '?'] options\n",
      "['What', 'you', 'can', 'do', '?'] options\n",
      "['What', 'help', 'you', 'provide', '?'] options\n",
      "['How', 'you', 'can', 'be', 'helpful', '?'] options\n",
      "['What', 'support', 'is', 'offered'] options\n",
      "['How', 'to', 'check', 'Adverse', 'drug', 'reaction', '?'] adverse_drug\n",
      "['Open', 'adverse', 'drugs', 'module'] adverse_drug\n",
      "['Give', 'me', 'a', 'list', 'of', 'drugs', 'causing', 'adverse', 'behavior'] adverse_drug\n",
      "['List', 'all', 'drugs', 'suitable', 'for', 'patient', 'with', 'adverse', 'reaction'] adverse_drug\n",
      "['Which', 'drugs', 'dont', 'have', 'adverse', 'reaction', '?'] adverse_drug\n",
      "['Open', 'blood', 'pressure', 'module'] blood_pressure\n",
      "['Task', 'related', 'to', 'blood', 'pressure'] blood_pressure\n",
      "['Blood', 'pressure', 'data', 'entry'] blood_pressure\n",
      "['I', 'want', 'to', 'log', 'blood', 'pressure', 'results'] blood_pressure\n",
      "['Blood', 'pressure', 'data', 'management'] blood_pressure\n",
      "['I', 'want', 'to', 'search', 'for', 'blood', 'pressure', 'result', 'history'] blood_pressure_search\n",
      "['Blood', 'pressure', 'for', 'patient'] blood_pressure_search\n",
      "['Load', 'patient', 'blood', 'pressure', 'result'] blood_pressure_search\n",
      "['Show', 'blood', 'pressure', 'results', 'for', 'patient'] blood_pressure_search\n",
      "['Find', 'blood', 'pressure', 'results', 'by', 'ID'] blood_pressure_search\n",
      "['Find', 'me', 'a', 'pharmacy'] pharmacy_search\n",
      "['Find', 'pharmacy'] pharmacy_search\n",
      "['List', 'of', 'pharmacies', 'nearby'] pharmacy_search\n",
      "['Locate', 'pharmacy'] pharmacy_search\n",
      "['Search', 'pharmacy'] pharmacy_search\n",
      "['Lookup', 'for', 'hospital'] hospital_search\n",
      "['Searching', 'for', 'hospital', 'to', 'transfer', 'patient'] hospital_search\n",
      "['I', 'want', 'to', 'search', 'hospital', 'data'] hospital_search\n",
      "['Hospital', 'lookup', 'for', 'patient'] hospital_search\n",
      "['Looking', 'up', 'hospital', 'details'] hospital_search\n"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes =[]\n",
    "documents = []\n",
    "ignore_list = ['.' , ',', '?', '!' ] # We will not consider these for our word list!\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #Tokenization\n",
    "        word = nltk.word_tokenize(pattern)\n",
    "        #Tokenization is just splitting the sentence/phrase into individual words\n",
    "        \n",
    "        words.extend(word)\n",
    "        print(word, intent['tag'])\n",
    "        #Add Documents in the Corpus\n",
    "        documents.append((word, intent['tag']))\n",
    "        #print(documents)\n",
    "        #Add to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "#print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatization (This will give us the Roots for all the words)\n",
    "words = [lemmatizer.lemmatize(w.lower())for w in words if w not in ignore_list]\n",
    "words = sorted(list(set(words)))\n",
    "#print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 Documents\n",
      "9 Classes/Intents:  ['adverse_drug', 'blood_pressure', 'blood_pressure_search', 'goodbye', 'greeting', 'hospital_search', 'options', 'pharmacy_search', 'thanks']\n",
      "87 Unique words (Post-Lemmatization) [\"'s\", 'a', 'adverse', 'all', 'anyone', 'are', 'awesome', 'be', 'behavior', 'blood', 'by', 'bye', 'can', 'causing', 'chatting', 'check', 'could', 'data', 'day', 'detail', 'do', 'dont', 'drug', 'entry', 'find', 'for', 'give', 'good', 'goodbye', 'have', 'hello', 'help', 'helpful', 'helping', 'hey', 'hi', 'history', 'hola', 'hospital', 'how', 'i', 'id', 'is', 'later', 'list', 'load', 'locate', 'log', 'looking', 'lookup', 'management', 'me', 'module', 'nearby', 'next', 'nice', 'of', 'offered', 'open', 'patient', 'pharmacy', 'pressure', 'provide', 'reaction', 'related', 'result', 'search', 'searching', 'see', 'show', 'suitable', 'support', 'task', 'thank', 'thanks', 'that', 'there', 'till', 'time', 'to', 'transfer', 'up', 'want', 'what', 'which', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "# Sorting Classes\n",
    "classes = sorted(list(set(classes)))\n",
    "print(len(documents), 'Documents')\n",
    "print(len(classes), 'Classes/Intents: ', classes)\n",
    "print(len(words), 'Unique words (Post-Lemmatization)', words)\n",
    "\n",
    "pickle.dump(words, open('words.pkl', 'wb'))\n",
    "pickle.dump(classes, open('classes.pkl', 'wb')) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training & Testing Data\n",
    "# Now we will convert the word list into Training Data!\n",
    "\n",
    "training = []\n",
    "output = [0]* len(classes)\n",
    "\n",
    "for doc in documents:\n",
    "    #Initializing Bag of Words\n",
    "    bag = []\n",
    "    \n",
    "    #List of tokenized words for the pattern\n",
    "    word_pattern = doc[0]\n",
    "    \n",
    "    #Lemmatizing each word \n",
    "    word_pattern = [lemmatizer.lemmatize(word.lower()) for word in word_pattern]\n",
    "    \n",
    "    #Create the bag of words array with 1, if word is found in current pattern\n",
    "    for word in words:\n",
    "        bag.append(1) if word in word_pattern else bag.append(0)\n",
    "    \n",
    "    # One Hot Encoding\n",
    "    output_row = list(output)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "#Shuffling the features\n",
    "random.shuffle(training)\n",
    "training = np.array(training, dtype=object)\n",
    "\n",
    "#Create training list. Train_x contains patterns, train_y contains Intents (That we are aiming to predict with our model)\n",
    "\n",
    "train_x = list(training[:, 0])\n",
    "train_y = list(training[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 128)               11264     \n",
      "_________________________________________________________________\n",
      "dropout_28 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_29 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 9)                 585       \n",
      "=================================================================\n",
      "Total params: 24,265\n",
      "Trainable params: 24,265\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Training \n",
    "# Constructing the Deep Neural Network Architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape = (len(train_x[0]), ), activation = 'relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(train_y[0]), activation = 'softmax'))\n",
    "\n",
    "#Compiling the model\n",
    "sgd = SGD(lr=0.01, decay = 1e-6, momentum = 0.9, nesterov = True)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer =sgd, metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 32 samples, validate on 15 samples\n",
      "Epoch 1/200\n",
      "32/32 [==============================] - 0s 8ms/step - loss: 2.2654 - accuracy: 0.0625 - val_loss: 2.1969 - val_accuracy: 0.2000\n",
      "Epoch 2/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.1797 - accuracy: 0.1250 - val_loss: 2.2126 - val_accuracy: 0.0667\n",
      "Epoch 3/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.1867 - accuracy: 0.1250 - val_loss: 2.2227 - val_accuracy: 0.0667\n",
      "Epoch 4/200\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.1009 - accuracy: 0.2188 - val_loss: 2.2249 - val_accuracy: 0.1333\n",
      "Epoch 5/200\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 2.0373 - accuracy: 0.3438 - val_loss: 2.2194 - val_accuracy: 0.1333\n",
      "Epoch 6/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 2.0133 - accuracy: 0.2812 - val_loss: 2.1950 - val_accuracy: 0.1333\n",
      "Epoch 7/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.9192 - accuracy: 0.2812 - val_loss: 2.1800 - val_accuracy: 0.0667\n",
      "Epoch 8/200\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 1.8777 - accuracy: 0.3438 - val_loss: 2.1481 - val_accuracy: 0.2000\n",
      "Epoch 9/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.7495 - accuracy: 0.4062 - val_loss: 2.1093 - val_accuracy: 0.2000\n",
      "Epoch 10/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.7080 - accuracy: 0.3750 - val_loss: 2.0454 - val_accuracy: 0.2000\n",
      "Epoch 11/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.5916 - accuracy: 0.4688 - val_loss: 1.9893 - val_accuracy: 0.1333\n",
      "Epoch 12/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 1.5254 - accuracy: 0.4688 - val_loss: 1.9445 - val_accuracy: 0.2000\n",
      "Epoch 13/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.3810 - accuracy: 0.5312 - val_loss: 1.9091 - val_accuracy: 0.2000\n",
      "Epoch 14/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.2584 - accuracy: 0.5000 - val_loss: 1.7558 - val_accuracy: 0.4667\n",
      "Epoch 15/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1443 - accuracy: 0.6875 - val_loss: 1.6309 - val_accuracy: 0.5333\n",
      "Epoch 16/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.9109 - accuracy: 0.7188 - val_loss: 1.5790 - val_accuracy: 0.5333\n",
      "Epoch 17/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.1342 - accuracy: 0.5625 - val_loss: 1.6281 - val_accuracy: 0.4667\n",
      "Epoch 18/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 1.0814 - accuracy: 0.6250 - val_loss: 1.6790 - val_accuracy: 0.5333\n",
      "Epoch 19/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8872 - accuracy: 0.6875 - val_loss: 1.6900 - val_accuracy: 0.4667\n",
      "Epoch 20/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7004 - accuracy: 0.7500 - val_loss: 1.6539 - val_accuracy: 0.4667\n",
      "Epoch 21/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7660 - accuracy: 0.7188 - val_loss: 1.5194 - val_accuracy: 0.6000\n",
      "Epoch 22/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8160 - accuracy: 0.7188 - val_loss: 1.4605 - val_accuracy: 0.5333\n",
      "Epoch 23/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.8040 - accuracy: 0.7812 - val_loss: 1.3447 - val_accuracy: 0.6000\n",
      "Epoch 24/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.7661 - accuracy: 0.7812 - val_loss: 1.3400 - val_accuracy: 0.6667\n",
      "Epoch 25/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5437 - accuracy: 0.7500 - val_loss: 1.3822 - val_accuracy: 0.5333\n",
      "Epoch 26/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5376 - accuracy: 0.8750 - val_loss: 1.4079 - val_accuracy: 0.4667\n",
      "Epoch 27/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5427 - accuracy: 0.8125 - val_loss: 1.3634 - val_accuracy: 0.6000\n",
      "Epoch 28/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5826 - accuracy: 0.7812 - val_loss: 1.3135 - val_accuracy: 0.6000\n",
      "Epoch 29/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4882 - accuracy: 0.8750 - val_loss: 1.2386 - val_accuracy: 0.6000\n",
      "Epoch 30/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4812 - accuracy: 0.8438 - val_loss: 1.2756 - val_accuracy: 0.5333\n",
      "Epoch 31/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3179 - accuracy: 0.8750 - val_loss: 1.2638 - val_accuracy: 0.6000\n",
      "Epoch 32/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.5010 - accuracy: 0.7812 - val_loss: 1.2748 - val_accuracy: 0.4667\n",
      "Epoch 33/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.4276 - accuracy: 0.8438 - val_loss: 1.4540 - val_accuracy: 0.5333\n",
      "Epoch 34/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2402 - accuracy: 0.9062 - val_loss: 1.5142 - val_accuracy: 0.5333\n",
      "Epoch 35/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3663 - accuracy: 0.8750 - val_loss: 1.3298 - val_accuracy: 0.4667\n",
      "Epoch 36/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2797 - accuracy: 0.9375 - val_loss: 1.1092 - val_accuracy: 0.6000\n",
      "Epoch 37/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.3082 - accuracy: 0.9375 - val_loss: 0.9689 - val_accuracy: 0.5333\n",
      "Epoch 38/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2502 - accuracy: 0.8438 - val_loss: 0.9694 - val_accuracy: 0.5333\n",
      "Epoch 39/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2665 - accuracy: 0.9688 - val_loss: 0.9974 - val_accuracy: 0.6000\n",
      "Epoch 40/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2094 - accuracy: 0.9688 - val_loss: 1.1471 - val_accuracy: 0.6667\n",
      "Epoch 41/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2388 - accuracy: 0.9375 - val_loss: 1.3121 - val_accuracy: 0.6000\n",
      "Epoch 42/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1976 - accuracy: 0.9688 - val_loss: 1.4213 - val_accuracy: 0.5333\n",
      "Epoch 43/200\n",
      "32/32 [==============================] - 0s 3ms/step - loss: 0.0995 - accuracy: 1.0000 - val_loss: 1.4835 - val_accuracy: 0.4667\n",
      "Epoch 44/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1743 - accuracy: 0.9375 - val_loss: 1.4970 - val_accuracy: 0.5333\n",
      "Epoch 45/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.0974 - accuracy: 0.9688 - val_loss: 1.5492 - val_accuracy: 0.6000\n",
      "Epoch 46/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1230 - accuracy: 0.9688 - val_loss: 1.7718 - val_accuracy: 0.5333\n",
      "Epoch 47/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.2643 - accuracy: 0.9688 - val_loss: 1.6590 - val_accuracy: 0.5333\n",
      "Epoch 48/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1714 - accuracy: 0.9375 - val_loss: 1.4278 - val_accuracy: 0.6000\n",
      "Epoch 49/200\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.1720 - accuracy: 0.9375 - val_loss: 1.2825 - val_accuracy: 0.6000\n",
      "Epoch 50/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1827 - accuracy: 0.9375 - val_loss: 1.1953 - val_accuracy: 0.6667\n",
      "Epoch 51/200\n",
      "32/32 [==============================] - 0s 1ms/step - loss: 0.2688 - accuracy: 0.9375 - val_loss: 1.1313 - val_accuracy: 0.6667\n",
      "Epoch 52/200\n",
      "32/32 [==============================] - 0s 2ms/step - loss: 0.1162 - accuracy: 0.9688 - val_loss: 1.1102 - val_accuracy: 0.6667\n"
     ]
    }
   ],
   "source": [
    "#We'll use the Keras Early Stopping, based on Validation Accuracy, and for the Train & Validation set split, we'll use the inbuilt feature of the fit method! \n",
    "early_stopper = EarlyStopping(min_delta = 0.05 , patience = 15)\n",
    "\n",
    "train_hist = model.fit(np.array(train_x), np.array(train_y),validation_split=0.3, epochs = 200, batch_size = 5, verbose = 1, callbacks = [early_stopper])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The validation accuracy isn't very good, but that's primarily becuase we are dealing with a very small dataset. \n",
    "# The purpose of this project was to understand how to handle text data and I'll be publishing more projects in the future to build models with large datasets, \n",
    "# since a pre-condition of working with Deep Learning Models is to have large amount of Data. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we'll save our trained model to be used in for the Chatbot!\n",
    "\n",
    "model.save('chat_bot_tr_model.h5', train_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
